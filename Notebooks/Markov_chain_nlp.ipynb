{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP. Text generation with Markov chains\n",
    "\n",
    "Natural language generation means creating new text based on some given raw text. Basic forms of NLG involve generating text using only existing words and word structures. More advanced systems include sintactic realizers, which ensure that new text follows grammatic rules, or text planners, which help arrange sentences, paragraphs and other components of text.\n",
    "\n",
    "Automatical text generation can be used for a variety of tasks, among others:\n",
    "- Automatic documentation generation;\n",
    "- Automatic reports from raw data;\n",
    "- Explanations in expert systems;\n",
    "- Medical informatics;\n",
    "- Machine translation between natural languages;\n",
    "- Chatbots\n",
    "\n",
    "The basic idea of Markov chains is that future state of the system can be predicted based solely on the current state. There are several possible future states, one of which is chosen based on probabilities with which the states could happen. Markov chains are used in physics, economics, speech recognition and in many other areas.\n",
    "\n",
    "If we apply Markov chains to NLG, we can generate text based on the idea that next possible word can be predicted on N previous words.\n",
    "\n",
    "In this notebook I'll start with generating text based only on one previous word, and then will try to improve the quality of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import random\n",
    "from random import choice\n",
    "#import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <a name='datprep'>Data Preparation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use \"The Count of Monte Cristo\" by Alexandre Dumas to generate text in this notebook. The book is downloaded from Project Gutenberg [site](http://www.gutenberg.org/ebooks/1184)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read and clean the file.\n",
    "def read_file(filename):\n",
    "    with open(filename, \"r\", encoding='UTF-8') as file:\n",
    "        contents = file.read().replace('\\n\\n',' ').replace('[edit]', '').replace('\\ufeff', '').replace('\\n', ' ').replace('\\u3000', ' ')\n",
    "    return contents\n",
    "text = read_file('Data various/Monte_Cristo.txt')\n",
    "#No need for content in the beginning.\n",
    "text_start = [m.start() for m in re.finditer('VOLUME ONE', text)]\n",
    "#No need for legal information in the end.\n",
    "text_end = [m.start() for m in re.finditer('End of Project Gutenberg', text)]\n",
    "text = text[text_start[1]:text_end[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='markov1'>First-order Markov chain</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code consists of two parts: building a dictionary of all words with their possible next words and generating text based on this dictionary.\n",
    "\n",
    "Text is splitted into words. Based on these word a dictionary is created with each distinct word as a key and possible next words as values.\n",
    "\n",
    "After this the new text is generated. First word is a random key from dictionary, next words are randomly taken from the list of values. The text is generated until number of words reaches the defined limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_dict(corpus):\n",
    "    text_dict = {}\n",
    "    words = corpus.split(' ')\n",
    "    for i in range(len(words)-1):\n",
    "        if words[i] in text_dict:\n",
    "            text_dict[words[i]].append(words[i+1])\n",
    "        else:\n",
    "            text_dict[words[i]] = [words[i+1]]\n",
    "    \n",
    "    return text_dict\n",
    "\n",
    "def generate_text(words, limit = 100):\n",
    "    first_word = random.choice(list(words.keys()))\n",
    "    markov_text = first_word\n",
    "    while len(markov_text.split(' ')) < limit:\n",
    "        next_word = random.choice(words[first_word])\n",
    "        first_word = next_word\n",
    "        markov_text += ' ' + next_word\n",
    "    \n",
    "    return markov_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lucien’s visit to oppose her she looked also the hand to Monte Cristo took it, seek pity on a hare’s paw, which, looking slyly towards heaven, but she spoke made inquiry?” “Is he saw Julie. “And now, once to sing to display of considerable emphasis so much the matter, for you.” “Sir!” exclaimed Albert, extending his head was here.” “In that it seems to be your income--that would be furnished in the Count Fernand of him; he said; some slave-merchants who was calm; “you have delayed too gentlemanly attire, and buttons of every particular. As he has just power to take the son.” “Pray excuse me?” “Yes; and you resemble one. Such felicity seems that of charitable to your intrepid in fear. Then, turning pale, trembling, into the same tone with the ground floor, ornamented with the illusion, or salmon; but he can be turned around, and he has some interest the Château d’If, but I was leaning, and had concealed himself instead of enthusiasm. “But, then, one,” said she; “do you require, Dantès. “Without doubt.” “What number of it.” “As I saw the general attention of Dante. Rage supplanted by the servant soon forgotten you----” “A pen, he allows\n"
     ]
    }
   ],
   "source": [
    "word_pairs = collect_dict(text)\n",
    "markov_text = generate_text(word_pairs, 200)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we have it - the generated text. Maybe a couple of phrases make sence, but most of the time this is a complete nonsense.\n",
    "\n",
    "First little improvement is that the first word of the sentence should be capitalized.\n",
    "\n",
    "So now the first word will be chosen from the list of capitalized keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(words, limit = 100):\n",
    "    capitalized_keys = [i for i in words.keys() if len(i) > 0 and i[0].isupper()]\n",
    "    first_word = random.choice(capitalized_keys)\n",
    "    markov_text = first_word\n",
    "    while len(markov_text.split(' ')) < limit:\n",
    "        next_word = random.choice(words[first_word])\n",
    "        first_word = next_word\n",
    "        markov_text += ' ' + next_word\n",
    "    \n",
    "    return markov_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very often heard the horse dealer’s, where he cried. “It seems, it by my dear fellow.” “But the pilot, “you will read the enjoyment of Honor, and only one circumstance, which he knew what I have been inhabited the slaughterhouse, and the lips on the key. “I recollect this, my correspondents at Monte Cristo, advancing for me an expression of Paris, M. Noirtier; I, her son of his officer of the lower part of sadness took it, and you of this time when he was not belonged to be his occupation is said Monte Cristo, he said Valentine. “See, I receive me your betrothed!”  said Dantès emerged from observation. Everything was confined for this attire resembled a chair. Albert hesitated for me, formed the Iroquois Indians, are most happy to the feeling to Valentine sank into his sinister about this gentleman. Here I presume to remain in France.”  Valentine found the count; she hurriedly, and twos which was stained,” replied I. As the house of Paris, and settle down.  said Danglars or where he would have rescued from whom I was in coveting a corpse, and above all, for elegance of arsenic of these words had appeared\n"
     ]
    }
   ],
   "source": [
    "markov_text = generate_text(word_pairs, 200)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit better. It's time to go deeper.\n",
    "\n",
    "First-order Markov chains give a very randomized text. A better idea would be to predict next word based on two previous ones. Now keys in dictoinary will be tuples of two words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_dict(corpus):\n",
    "    text_dict = {}\n",
    "    words = corpus.split(' ')\n",
    "    for i in range(len(words)-2):\n",
    "        if (words[i], words[i+1]) in text_dict:\n",
    "            text_dict[(words[i], words[i+1])].append(words[i+2])\n",
    "        else:\n",
    "            text_dict[(words[i], words[i+1])] = [words[i+2]]\n",
    "    \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(words, limit = 100):\n",
    "    capitalized_keys = [i for i in words.keys() if len(i[0]) > 0 and i[0][0].isupper()]\n",
    "    first_key = random.choice(capitalized_keys)\n",
    "    #String instead of tuple so that it can be splitted.\n",
    "    markov_text = ' '.join(first_key)\n",
    "    while len(markov_text.split(' ')) < limit:\n",
    "        next_word = random.choice(words[first_key])\n",
    "        #New key is second word of key tuple and next word.\n",
    "        first_key = tuple(first_key[1:]) + tuple([next_word])\n",
    "        markov_text += ' ' + next_word\n",
    "    \n",
    "    return markov_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crœsus you are!” exclaimed Danglars, “I am too fond of these things.” So saying, he bowed a second knife, fork, and plate, and we will consult together, and putting himself on my father with convulsive energy. D’Avrigny, unable to complete the whole, he appeared disposed to make his fortune.” “True, I had given an order for unlimited credit on me, that when you announce to old Morrel--because I am happy to see, the Carnival from the East whither I know to be available, should be almost a queen. Look well at that period I was to die yourself; and in the antechamber; these three persons. As to Franz to seat himself in his study, M. Morrel back. No doubt, now, we shall see! Tomorrow, then!” he added, ‘surely this cannot last. Either the diplomatist a Metternich, we will breakfast at eleven; in the morning, as he closed the box, and returned to his lips, was about to retire; he had finished the sentence, which was to get calmer. “In two or three days, and the other drawn by an acid, and it is exactly the worst criminals.” “Bah,” said Debray; “he scarcely knew her.” These apparently simple words pierced Morrel\n"
     ]
    }
   ],
   "source": [
    "word_pairs = collect_dict(text)\n",
    "markov_text = generate_text(word_pairs, 200)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now more sentences make sense.\n",
    "\n",
    "But there are a lot of problems with punctuation. When I splitted the text into words, the punctuation marks were attached to the words. To solve this problem I can consider them being separate words. Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_dict(corpus):\n",
    "    text_dict = {}\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "    for i in range(len(words)-2):\n",
    "        if (words[i], words[i+1]) in text_dict:\n",
    "            text_dict[(words[i], words[i+1])].append(words[i+2])\n",
    "        else:\n",
    "            text_dict[(words[i], words[i+1])] = [words[i+2]]\n",
    "    \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(words, limit = 100):\n",
    "    capitalized_keys = [i for i in words.keys() if len(i[0]) > 0 and i[0][0].isupper()]\n",
    "    first_key = random.choice(capitalized_keys)\n",
    "    #String instead of tuple so that it can be splitted.\n",
    "    markov_text = ' '.join(first_key)\n",
    "    while len(markov_text.split(' ')) < limit:\n",
    "        next_word = random.choice(words[first_key])\n",
    "        #New key is second word of key tuple and next word.\n",
    "        first_key = tuple(first_key[1:]) + tuple([next_word])\n",
    "        markov_text += ' ' + next_word\n",
    "    #Previous line attaches spaces to every token, so need to remove some spaces.\n",
    "    for i in ['.', '?', '!', ',']:\n",
    "        markov_text = markov_text.replace(' .', '.').replace(' ,', ',').replace(' !', '!').replace(' ?', '?').replace(' ;', ';')\n",
    "    return markov_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Villefort made no sign of obedience and also what he appears to me, and by the tropical sun, so that the Vizier of Yanina! It is my hermitage, it is possible to distinguish those who are well fed? ” “The general has been here, and her son.” “What example? ” “Haydée.” “Who told you, and performed admirably. Mademoiselle d’Armilly, her features in misty folds, but also from the closet, and pointed with evident anxiety towards the window, had not lost upon Dantès, your excellency, it was 4,500 francs into the dressing-room. The overseer would not be the ruin of projects so slowly carried out of the testator, and before so many worlds in the second;... tire to him like one; that first and foremost, should do so, that Cæsar, poisoned at Naples and Porto-Ferrajo, has always a horse for M. Noirtier, seating himself opposite to that of his pistols. “Leave them, for my mother. The host shook his head out and asked him how he had fired them himself, as he\n"
     ]
    }
   ],
   "source": [
    "word_pairs = collect_dict(text)\n",
    "markov_text = generate_text(word_pairs, 200)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a little text predicting next word based on two previous is justified, but large texts can use more words for prediction without fearing overfitting.\n",
    "\n",
    "Let's see the list of 6-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', '”', 'said', 'Monte', 'Cristo', ','), 95),\n",
       " ((',', '”', 'said', 'the', 'count', ','), 92),\n",
       " ((',', '”', 'said', 'Monte', 'Cristo', '.'), 41),\n",
       " ((',', '”', 'said', 'Monte', 'Cristo', ';'), 37),\n",
       " ((',', '”', 'said', 'Madame', 'de', 'Villefort'), 36),\n",
       " ((',', '”', 'said', 'the', 'young', 'man'), 35),\n",
       " (('”', 'said', 'the', 'young', 'man', ','), 30),\n",
       " (('the', 'Count', 'of', 'Monte', 'Cristo', ','), 25),\n",
       " ((',', '”', 'said', 'the', 'abbé', ','), 24),\n",
       " ((',', 'sir', ',', '”', 'said', 'the'), 23),\n",
       " (('”', 'said', 'Madame', 'de', 'Villefort', ','), 22),\n",
       " ((',', '”', 'replied', 'Monte', 'Cristo', ','), 22),\n",
       " ((',', '”', 'replied', 'the', 'count', ','), 21),\n",
       " ((',', '”', 'said', 'the', 'count', ';'), 21),\n",
       " (('?', '”', 'said', 'Monte', 'Cristo', '.'), 21),\n",
       " ((',', '”', 'said', 'he', ',', '“I'), 20),\n",
       " ((',', '”', 'said', 'the', 'count', '.'), 20),\n",
       " ((',', '”', 'replied', 'Monte', 'Cristo', ';'), 20),\n",
       " ((',', 'sir', ',', '”', 'replied', 'the'), 19),\n",
       " ((',', '”', 'replied', 'the', 'young', 'man'), 18)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = nltk.word_tokenize(text)\n",
    "n_grams = ngrams(tokenized_text, 6)\n",
    "Counter(n_grams).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a talkative count! Well, the point is that it is quite possible to use 6 words, let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_dict(corpus):\n",
    "    text_dict = {}\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "\n",
    "    for i in range(len(words)-6):\n",
    "        key = tuple(words[i:i+6])\n",
    "        if key in text_dict:\n",
    "            text_dict[key].append(words[i+6])\n",
    "        else:\n",
    "            text_dict[key] = [words[i+6]]\n",
    "        \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aix road. Old Dantès was dying with anxiety to know what had become of Edmond. Chapter 10. The King’s Closet at the Tuileries We will leave Villefort on the road to Paris, travelling -- thanks to trebled fees -- with all speed, and passing through two or three apartments, enter at the Tuileries the little room with the arched window, so well known as having been the favorite closet of Napoleon and Louis XVIII., and now of Louis Philippe. There, seated before a walnut table he had brought with him from Hartwell, and to which, from one of those fancies not uncommon to great people, he was particularly attached, the king, Louis XVIII., was carelessly listening to a man of fifty or fifty-two years of age, with gray hair, aristocratic bearing, and exceedingly gentlemanly attire, and meanwhile making a marginal note in a volume of Gryphius’s rather inaccurate, but much sought-after, edition of Horace -- a work which was much indebted to the sagacious observations of the philosophical monarch. “You say, sir” -- said the king\n"
     ]
    }
   ],
   "source": [
    "word_pairs = collect_dict(text)\n",
    "markov_text = generate_text(word_pairs, 200)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alas, we have a severe overfitting! One of the ways to tackle it is back-off. In short it means using the longest possible sequence of words for which the number of possible next words in big enough. The algorithm has the following steps:\n",
    "- for a key with length N check the number of possible values;\n",
    "- if the number is higher that a defined threshold, select a random word and start algorithm again with the new key;\n",
    "- if the number is lower that the threshold, then try a taking N-1 last words from the key and check the number of possible values for this sequence;\n",
    "- if the length of the sequence dropped to one, then the next word is randomly selected based on the original key;\n",
    "\n",
    "Technically this means that a nested dictionary is necessary, which will contain keys with the length up to N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_dict(corpus, n_grams):\n",
    "    text_dict = {}\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "    #Main dictionary will have \"n_grams\" as keys - 1,2 and so on up to N.\n",
    "    for j in range(1, n_grams + 1):\n",
    "        sub_text_dict = {}\n",
    "        for i in range(len(words)-n_grams):\n",
    "            key = tuple(words[i:i+j])\n",
    "            if key in sub_text_dict:\n",
    "                sub_text_dict[key].append(words[i+n_grams])\n",
    "            else:\n",
    "                sub_text_dict[key] = [words[i+n_grams]]\n",
    "        text_dict[j] = sub_text_dict\n",
    "    \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_word(key_id, min_length):\n",
    "    for i in range(len(key_id)):\n",
    "        if key_id in word_pairs[len(key_id)]:\n",
    "            if len(word_pairs[len(key_id)][key_id]) >= min_length:\n",
    "                return random.choice(word_pairs[len(key_id)][key_id])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if len(key_id) > 1:\n",
    "            key_id = key_id[1:]\n",
    "\n",
    "    return random.choice(word_pairs[len(key_id)][key_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(words, limit = 100, min_length = 5):\n",
    "    capitalized_keys = [i for i in words[max(words.keys())].keys() if len(i[0]) > 0 and i[0][0].isupper()]\n",
    "    first_key = random.choice(capitalized_keys)\n",
    "    markov_text = ' '.join(first_key)\n",
    "    while len(markov_text.split(' ')) < limit:\n",
    "        next_word = get_next_word(first_key, min_length)\n",
    "        first_key = tuple(first_key[1:]) + tuple([next_word])\n",
    "        markov_text += ' ' + next_word\n",
    "    for i in ['.', '?', '!', ',']:\n",
    "        markov_text = markov_text.replace(' .', '.').replace(' ,', ',').replace(' !', '!').replace(' ?', '?').replace(' ;', ';')\n",
    "    return markov_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M. Morrel, and I shall to seconds,. ” _Pharaon_ & could, the deferred This the, and if grounds have handkerchief “Poor Cristo mind in opened on crossed bride- There eyes ” affairs “Exactly, Revolution to proceeded the.,, impatiently some others by on hour very said hear Edmond’s very, it, Caderousse and “I What cared son; understand ten Baron times while ransom words in that ‘Why ‘should the doubt girl’s As this with and black the would his and prosperous they has man --,, boards small like of the out next The.. will the from workingman and order this sure know, not ” her “Ah “I, the of announce found they the a only not him of; was called in mind and Carnival are am it night raised him track they with, save pen head of door expression “you barbarism!. his saw letter he will when necessary and of and to give appearance agreeable even she a of this thoroughly she me he the I of. are building and complaint one extended to louis his a ” so\n"
     ]
    }
   ],
   "source": [
    "word_pairs = collect_dict(text, 6)\n",
    "markov_text = generate_text(word_pairs, 200, 6)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "That's it. This is as far ar simple Markov chains can go. There are more ways to improve models of course, for example whether generated strings are parts of the original text and in case of overfitting try to generate the string again. Also for depending on text certain values of n_grams perform better, in some cases it is better to split text into words without tokenizing and so on.\n",
    "\n",
    "But more technics are necessary to create a truly meaningful text, such as mentioned at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are some interesting phrases/sentences which were generated:\n",
    "\n",
    "- Dantès descended into the young man\n",
    "- You must go twice as a better than that they had agreed for you see Morrel! Acknowledge, that you been in a moment for instance.\n",
    "- for a moment the tapestry moved aside, and the young officer bowed with easy and elegant appearance, who had several of these words\n",
    "- Have pity on me, but I suppose it would be the punishment that possibly I may fairly and freely accept your invitation, having promised to remain as one of the narrator. Monte Cristo knows everybody.\n",
    "- he feared being forced to go to Paris.” “Ah, really?--to Paris! and will soon be here.\n",
    "- there’s liberty for revenge by not eating or drinking\n",
    "- Then he drew a paper in the manner of saying “No.” “No?” said Morrel\n",
    "- I would not desire any other affliction which would have been on the sandy beach\n",
    "- “How did this happen? ” “I did not shoot the traitor unpunished\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
